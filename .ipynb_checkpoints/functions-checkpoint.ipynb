{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将每一列标准化\n",
    "from sklearn import preprocessing\n",
    "def standardize(X, wholeImage=False):\n",
    "    if wholeImage:\n",
    "        scaled = np.zeros(X.shape, dtype=float)\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(3):\n",
    "                img = X[i, :, :, j]\n",
    "                scaled[i, :, :, j] = preprocessing.scale(img)\n",
    "        return scaled\n",
    "    return preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from os import listdir, path\n",
    "from os.path import isfile, join\n",
    "import matplotlib.image as mpimg\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "from skimage.color import rgb2gray\n",
    "from scipy.ndimage import zoom\n",
    "def get_data(img_folder, data_path, num_img_per_leaf, test_split, class_names, trailingName, color=True):\n",
    "    \n",
    "    # 文件是否已存在\n",
    "    if path.exists(data_path):\n",
    "        return pickle.load( open( data_path, \"rb\" ) )\n",
    "        \n",
    "    # 存储数据的map\n",
    "    data = {} # 'train'/'test' : className : leafNumber : ID : 256x256x3 image\n",
    "    data['train'] = {}\n",
    "    data['test'] = {}\n",
    "    \n",
    "    # 得到各个分类的文件名\n",
    "    for class_name in class_names:\n",
    "        print('fetching files from', class_name)\n",
    "        \n",
    "        # 加入到map中\n",
    "        data['train'][class_name] = {}\n",
    "        data['test'][class_name] = {}\n",
    "\n",
    "        temp_map = {}\n",
    "\n",
    "        this_folder = img_folder + '/' + class_name\n",
    "\n",
    "        # filename列表\n",
    "        file_names = [f for f in listdir(this_folder) if isfile(join(this_folder, f))]\n",
    "        \n",
    "        # 拿到ID\n",
    "        for file_name in file_names:\n",
    "            if \"seg\" not in file_name:\n",
    "                ID = int(file_name[file_name.find(\"_\") + 1:(len(file_name) - len(trailingName))])\n",
    "                temp_map[ID] = file_name \n",
    "\n",
    "        # 排序\n",
    "        nLeaf = 0\n",
    "        tempC = 0\n",
    "        leaf_map = {}\n",
    "        map_ID_fileName = OrderedDict()\n",
    "        for ID in sorted (temp_map.keys()):\n",
    "            map_ID_fileName[ID] = temp_map[ID]\n",
    "            leaf_map[ID] = nLeaf\n",
    "            tempC += 1\n",
    "            if tempC - num_img_per_leaf == 0:\n",
    "                tempC = 0\n",
    "                nLeaf += 1\n",
    "\n",
    "        num_leaves = int(test_split * len(file_names) / num_img_per_leaf)\n",
    "\n",
    "        # 图像数\n",
    "        num_images = num_img_per_leaf * num_leaves\n",
    "\n",
    "        # 测试集\n",
    "        test_set = []\n",
    "\n",
    "        # 根据test_split分割出测试集\n",
    "        image_counter = 0\n",
    "        for ID in map_ID_fileName:\n",
    "            test_set.append(ID)\n",
    "            image_counter += 1\n",
    "            if image_counter >= num_images:\n",
    "                break\n",
    "\n",
    "        for ID in map_ID_fileName:\n",
    "            \n",
    "            if color:\n",
    "                img = mpimg.imread(this_folder + \"/\" +  map_ID_fileName[ID]) / 255.\n",
    "            else:\n",
    "                img = rgb2gray(mpimg.imread(this_folder + \"/\" +  map_ID_fileName[ID]) / 255.)\n",
    "            \n",
    "            leaf = leaf_map[ID]\n",
    "            dataset = 'train'\n",
    "            if ID in test_set:\n",
    "                dataset = 'test'\n",
    "            if leaf not in data[dataset][class_name]:\n",
    "                data[dataset][class_name][leaf] = {}\n",
    "            data[dataset][class_name][leaf][ID] = img\n",
    "\n",
    "    pickle.dump(data, open(data_path, \"wb\" ) )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机选取图片展示\n",
    "from math import sqrt\n",
    "from random import randrange\n",
    "def viewRandomImages(data, nImages, color=True):\n",
    "    # data[className][leafNumber][ID]\n",
    "    plt.figure(figsize=(13,10))\n",
    "    for i in range(nImages):\n",
    "        plt.subplot(sqrt(nImages),sqrt(nImages),i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        rClass = [key for key in data.keys()][randrange(len(data.keys()))]\n",
    "        rLeaf = [key for key in data[rClass].keys()][randrange(len(data[rClass].keys()))]\n",
    "        rID = [key for key in data[rClass][rLeaf].keys()][randrange(len(data[rClass][rLeaf].keys()))]\n",
    "        rImg = data[rClass][rLeaf][rID]\n",
    "        if color:\n",
    "            plt.imshow(rImg)\n",
    "        else:\n",
    "            plt.imshow(rImg, cmap=plt.cm.gray)\n",
    "        plt.xlabel(rClass + '_' + str(rID))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor_filters(filter_path):\n",
    "    \n",
    "    if path.exists(filter_path):\n",
    "        kernels = pickle.load( open( filter_path, \"rb\" ) )\n",
    "    \n",
    "    # 创建kernals\n",
    "    else:\n",
    "        kernels = []\n",
    "        for theta in range(4):\n",
    "            theta = theta / 4. * np.pi\n",
    "            for offset in range(4):\n",
    "                offset = offset / 4. * np.pi\n",
    "                for sigma in (1, 3):\n",
    "                    for freq in (0.05, 0.25):\n",
    "                        kernel = np.real(gabor_kernel(freq, offset=offset, theta=theta, sigma_x=sigma, sigma_y=sigma))\n",
    "                        kernels.append(kernel)\n",
    "        pickle.dump(kernels, open(filter_path, \"wb\" ) )\n",
    "    \n",
    "    # plot kernels\n",
    "    i=0\n",
    "    plt.figure(figsize=(len(kernels), len(kernels)))\n",
    "    for kernel in kernels:\n",
    "        plt.subplot(sqrt(len(kernels)) ,sqrt(len(kernels)),i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(kernel, cmap=plt.cm.gray)\n",
    "        i+=1\n",
    "    plt.show()\n",
    "    \n",
    "    return kernels\n",
    "\n",
    "\n",
    "# 将滤波器应用到图像\n",
    "from scipy import ndimage as ndi\n",
    "from skimage import data\n",
    "from skimage.util import img_as_float\n",
    "from skimage.filters import gabor_kernel\n",
    "from IPython.display import display, clear_output\n",
    "def apply_filters(data, data_path, filters, color=True):\n",
    "    \n",
    "    if path.exists(data_path):\n",
    "        return pickle.load( open( data_path, \"rb\" ) )\n",
    "    \n",
    "    filteredData = {}\n",
    "    for dataset in data:\n",
    "        filteredData[dataset] = {}\n",
    "        for className in data[dataset]:\n",
    "            dis = display('aplying filters to: ' + dataset + ', ' + className, display_id=True)\n",
    "            filteredData[dataset][className] = {}\n",
    "            for idx, leaf in enumerate(data[dataset][className]):\n",
    "                dis.update('aplying filters to: ' + dataset + ', ' + className \n",
    "                           + f' {100. * float(idx) / float(len(data[dataset][className])):.0f}' + \"%\")\n",
    "                filteredData[dataset][className][leaf] = {}\n",
    "                for ID in data[dataset][className][leaf]:\n",
    "                    img = data[dataset][className][leaf][ID]\n",
    "                    temp = [None] * (3 * len(filters))\n",
    "                    for f, ffilter in enumerate(filters):\n",
    "                        filtered = np.empty(1)\n",
    "                        if color:\n",
    "                            for i in range(3):\n",
    "                                filtered = np.append(filtered, ndi.convolve(img[:,:,i], ffilter, mode='wrap'))\n",
    "                        else:\n",
    "                            filtered = ndi.convolve(img, ffilter, mode='wrap')\n",
    "                        temp[3 * f] = filtered.sum()\n",
    "                        temp[3 * f + 1] = filtered.mean()\n",
    "                        temp[3 * f + 2] = filtered.var()\n",
    "                    filteredData[dataset][className][leaf][ID] = temp\n",
    "            dis.update('filters applied to: ' + dataset + ', ' + className)\n",
    "                    \n",
    "    pickle.dump(filteredData, open(data_path, \"wb\" ) )\n",
    "    return filteredData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_machine_learning(data, data_path):\n",
    "    \n",
    "    if path.exists(data_path):\n",
    "        return pickle.load( open( data_path, \"rb\" ) )\n",
    "    \n",
    "    # get dimensions\n",
    "    nTrain = 0\n",
    "    nTest = 0\n",
    "    nFeatures = 0\n",
    "    for dataset in data:\n",
    "        for className in data[dataset]:\n",
    "            for leaf in data[dataset][className]:\n",
    "                if nFeatures == 0:\n",
    "                    nFeatures = len(next(iter(data[dataset][className][leaf].values())))\n",
    "                if dataset == 'train':\n",
    "                    nTrain += len(data[dataset][className][leaf])\n",
    "                else:\n",
    "                    nTest += len(data[dataset][className][leaf])\n",
    "\n",
    "    # create ml data\n",
    "    mlData = {}\n",
    "    mlData['X_train'] = np.zeros((nTrain, nFeatures), dtype=float)\n",
    "    mlData['Y_train'] = [None] * nTrain\n",
    "    mlData['L_train'] = [None] * nTrain\n",
    "    mlData['ID_train'] = [None] * nTrain\n",
    "    mlData['X_test'] = np.zeros((nTest, nFeatures), dtype=float)\n",
    "    mlData['Y_test'] = [None] * nTest\n",
    "    mlData['L_test'] = [None] * nTest\n",
    "    mlData['ID_test'] = [None] * nTrain\n",
    "    train_idx = 0\n",
    "    test_idx = 0\n",
    "    for dataset in data:\n",
    "        for className in data[dataset]:\n",
    "            for leaf in data[dataset][className]:\n",
    "                for ID in data[dataset][className][leaf]:\n",
    "                    x = data[dataset][className][leaf][ID]\n",
    "                    if dataset == 'train':\n",
    "                        for idx, x_i in enumerate(x):\n",
    "                            mlData['X_train'][train_idx][idx] = x_i\n",
    "                        mlData['Y_train'][train_idx] = className\n",
    "                        mlData['L_train'][train_idx] = leaf\n",
    "                        mlData['ID_train'][train_idx] = ID\n",
    "                        train_idx += 1\n",
    "                    else:\n",
    "                        for idx, x_i in enumerate(x):\n",
    "                            mlData['X_test'][test_idx][idx] = x_i\n",
    "                        mlData['Y_test'][test_idx] = className\n",
    "                        mlData['L_test'][test_idx] = leaf\n",
    "                        mlData['ID_test'][test_idx] = ID\n",
    "                        test_idx += 1\n",
    "                    \n",
    "    mlData['X_train'] = standardize(mlData['X_train'])\n",
    "    mlData['X_test'] = standardize(mlData['X_test'])\n",
    "    \n",
    "    pickle.dump(mlData, open(data_path, \"wb\" ), protocol=4 )\n",
    "    return mlData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跑各种机器学习方法并获得准确度结果\n",
    "import warnings\n",
    "def mla_class_results(mlas, X, Y, L, results_path, disp=True):\n",
    "    \n",
    "    if path.exists(results_path):\n",
    "        results = pickle.load( open( results_path, \"rb\" ) )\n",
    "        \n",
    "    else:\n",
    "        results = {}\n",
    "\n",
    "        warnings.filterwarnings('ignore') # 忽略模型未收敛时的警告\n",
    "\n",
    "        for mla in mlas:\n",
    "            results[mla] = cross_validation_class(mla, X, Y, L, disp)\n",
    "\n",
    "        pickle.dump(results, open(results_path, \"wb\" ) )\n",
    "       \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: 训练集 Y: 类标签 nFolds: 交叉验证的fold个数\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from random import randint\n",
    "from sklearn.metrics import accuracy_score\n",
    "def cross_validation(mla, X, Y, L):\n",
    "    leafs = list(set(L))\n",
    "    Y = np.array(Y)\n",
    "    nFolds = 3\n",
    "    nPer = int(len(leafs) / nFolds)\n",
    "    pred_Y = []\n",
    "    test_Y = []\n",
    "    for f in range(nFolds):\n",
    "        fLeafs = []\n",
    "        if f < nFolds - 1:\n",
    "            for r in range(nPer):\n",
    "                rIdx = randint(0, len(leafs)-1)\n",
    "                rLeaf = leafs[rIdx]\n",
    "                fLeafs.append(rLeaf)\n",
    "                leafs.pop(rIdx)\n",
    "        else:\n",
    "            fLeafs = leafs\n",
    "        fRows = []\n",
    "        for fLeaf in fLeafs:\n",
    "            for idx, l in enumerate(L):\n",
    "                if l == fLeaf:\n",
    "                    fRows.append(idx)\n",
    "        \n",
    "        mla.fit(np.delete(X, fRows, 0), np.delete(Y, fRows, 0))\n",
    "        pred_Y = np.concatenate((pred_Y, mla.predict(np.take(X, fRows, 0))))\n",
    "        test_Y = np.concatenate((test_Y, np.take(Y, fRows, 0)))\n",
    "    return accuracy_score(test_Y, pred_Y)\n",
    "\n",
    "def cross_validation_class(name, X, Y, L, disp=True):\n",
    "    \n",
    "    mla = DecisionTreeClassifier()\n",
    "    \n",
    "    # 持续刷新结果\n",
    "    means = [None] * 100\n",
    "    \n",
    "    # get results\n",
    "    if disp:\n",
    "        dis = display('Training ML: ' + name, display_id=True)\n",
    "    for i in range(100):\n",
    "        if disp:\n",
    "            dis.update('Training ML: ' + name + f' {i:.0f}%')\n",
    "        if 'DTr' in name:\n",
    "            mla = DecisionTreeClassifier(random_state = i)\n",
    "        elif 'LRe' in name:\n",
    "            mla = LogisticRegression()\n",
    "        elif 'NBa' in name:\n",
    "            mla = GaussianNB()\n",
    "        elif 'RFo' in name:\n",
    "            mla = RandomForestClassifier(random_state = i, n_estimators=100)\n",
    "        elif 'ETr' in name:\n",
    "            mla = ExtraTreesClassifier(random_state = i, n_estimators=100)\n",
    "        elif 'SVM' in name:\n",
    "            mla = SVC(random_state = i, gamma='auto', probability=True)\n",
    "        elif 'KNN' in name:\n",
    "            mla = KNeighborsClassifier(n_neighbors=2)\n",
    "        elif 'MLP' in name:\n",
    "            mla = MLPClassifier(random_state = i, hidden_layer_sizes=[32], max_iter=10000)\n",
    "            \n",
    "        means[i] = cross_validation(mla, X, Y, L)\n",
    "        \n",
    "    if disp:\n",
    "        dis.update('Trained ML: ' + name + ' with ' + \"{:.2f}\".format(sum(means) * 100 / len(means)) + '% average accuracy')\n",
    "        \n",
    "    return means"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
