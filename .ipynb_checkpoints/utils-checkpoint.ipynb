{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple timer to time events\n",
    "import time\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "        \n",
    "    def get(self):\n",
    "        value = time.time() - self.start\n",
    "        self.start = time.time()\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert strings to ints\n",
    "import numpy as np\n",
    "def oneHotEncode(Y, classNames):\n",
    "    Y_new = [None] * len(Y)\n",
    "    for i in range(len(Y)):\n",
    "        Y_new[i] = int(classNames.index(Y[i]))\n",
    "    return Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ints to strings\n",
    "def reverseOneHotEncode(Y, classNames):\n",
    "    Y_new = [None] * len(Y)\n",
    "    for i in range(len(Y)):\n",
    "        Y_new[i] = classNames[Y[i]]\n",
    "    return Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize each column\n",
    "from sklearn import preprocessing\n",
    "def standardize(X, wholeImage=False):\n",
    "    if wholeImage:\n",
    "        scaled = np.zeros(X.shape, dtype=float)\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(3):\n",
    "                img = X[i, :, :, j]\n",
    "                scaled[i, :, :, j] = preprocessing.scale(img)\n",
    "        return scaled\n",
    "    return preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pearson correlation coefficients between two array\n",
    "from scipy.stats import pearsonr\n",
    "def correlations(X, Y, classNames, thresh):\n",
    "    Y = np.array(oneHotEncode(Y, classNames))\n",
    "    print(Y)\n",
    "    nFeatures = X.shape[1]\n",
    "    rs = [None] * X.shape[1]\n",
    "    cols = []\n",
    "    for i in range(nFeatures):\n",
    "        rs[i] = pearsonr(X[:, i:i+1].flatten(), Y)[0]\n",
    "        if abs(rs[i]) < thresh:\n",
    "            cols.append(i)\n",
    "    plt.scatter([x for x in range(nFeatures)], rs)\n",
    "    plt.xlabel('Feature #')\n",
    "    plt.ylabel('Pearson Correlation')\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a numpy array image and displays\n",
    "# optionaly set gray=True to print image in grayscale\n",
    "from matplotlib import pyplot as plt\n",
    "def showImage(img):\n",
    "    plt.imshow(img, cmap=plt.cm.gray)\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab some random indicies of pictures to view\n",
    "# pass in data['train'] or data['test']\n",
    "from math import sqrt\n",
    "from random import randrange\n",
    "def viewFilters(filters):\n",
    "    # data[className][leafNumber][ID]\n",
    "    #plt.figure(figsize=(10,10))\n",
    "    for i in range(len(filters)):\n",
    "        plt.subplot(sqrt(nImages),sqrt(nImages),i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        rClass = [key for key in data.keys()][randrange(len(data.keys()))]\n",
    "        rLeaf = [key for key in data[rClass].keys()][randrange(len(data[rClass].keys()))]\n",
    "        rID = [key for key in data[rClass][rLeaf].keys()][randrange(len(data[rClass][rLeaf].keys()))]\n",
    "        rImg = data[rClass][rLeaf][rID]\n",
    "        plt.imshow(rImg, cmap=plt.cm.gray)\n",
    "        plt.xlabel(rClass + '_' + str(rID))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab some random indicies of pictures to view\n",
    "# pass in data['train'] or data['test']\n",
    "from math import sqrt\n",
    "from random import randrange\n",
    "def viewRandomImages(data, nImages, color=True):\n",
    "    # data[className][leafNumber][ID]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(nImages):\n",
    "        plt.subplot(sqrt(nImages),sqrt(nImages),i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        rClass = [key for key in data.keys()][randrange(len(data.keys()))]\n",
    "        rLeaf = [key for key in data[rClass].keys()][randrange(len(data[rClass].keys()))]\n",
    "        rID = [key for key in data[rClass][rLeaf].keys()][randrange(len(data[rClass][rLeaf].keys()))]\n",
    "        rImg = data[rClass][rLeaf][rID]\n",
    "        if color:\n",
    "            plt.imshow(rImg)\n",
    "        else:\n",
    "            plt.imshow(rImg, cmap=plt.cm.gray)\n",
    "        plt.xlabel(rClass + '_' + str(rID))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a nested dictionary to store file names: \n",
    "#  maps a key <class_name> to sub-key <4-digit ID> to the value <file_name>\n",
    "#  second nested level with sub-key <4-digit ID> will be sorted from smallest ID to largest\n",
    "from collections import OrderedDict\n",
    "from os import listdir, path\n",
    "from os.path import isfile, join\n",
    "import matplotlib.image as mpimg\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "from skimage.color import rgb2gray\n",
    "from scipy.ndimage import zoom\n",
    "def getData(img_folder, data_path, num_img_per_leaf, test_split, class_names, trailingName, color=True):\n",
    "    \n",
    "    # check if file exists already\n",
    "    if path.exists(data_path):\n",
    "        return pickle.load( open( data_path, \"rb\" ) )\n",
    "        \n",
    "    # create map to fill with data\n",
    "    data = {} # 'train' or 'test' : className : leafNumber : ID : 256x256x3 image\n",
    "    data['train'] = {}\n",
    "    data['test'] = {}\n",
    "    \n",
    "    # get file names for each classification\n",
    "    for class_name in class_names:\n",
    "        print('fetching files from', class_name)\n",
    "        \n",
    "        # add to map\n",
    "        data['train'][class_name] = {}\n",
    "        data['test'][class_name] = {}\n",
    "\n",
    "        # create a temporary dictionary that will be sorted by ID later\n",
    "        temp_map = {}\n",
    "\n",
    "        # get folder where images are saved for this class\n",
    "        this_folder = img_folder + '/' + class_name\n",
    "\n",
    "        # get a list of file names in this folder\n",
    "        file_names = [f for f in listdir(this_folder) if isfile(join(this_folder, f))]\n",
    "        \n",
    "        # get 4-digit ID from each file name, and save in map\n",
    "        for file_name in file_names:\n",
    "            ID = int(file_name[len(file_name)-len(trailingName)-4:len(file_name)-len(trailingName)])\n",
    "            temp_map[ID] = file_name \n",
    "\n",
    "        # sort dictionary\n",
    "        nLeaf = 0\n",
    "        tempC = 0\n",
    "        leaf_map = {}\n",
    "        map_ID_fileName = OrderedDict()\n",
    "        for ID in sorted (temp_map.keys()):\n",
    "            map_ID_fileName[ID] = temp_map[ID]\n",
    "            leaf_map[ID] = nLeaf\n",
    "            tempC += 1\n",
    "            if tempC - num_img_per_leaf == 0:\n",
    "                tempC = 0\n",
    "                nLeaf += 1\n",
    "\n",
    "        # get number of leaves to pull for this classification\n",
    "        num_leaves = int(test_split * len(file_names) / num_img_per_leaf)\n",
    "\n",
    "        # get number of images to pull\n",
    "        num_images = num_img_per_leaf * num_leaves\n",
    "\n",
    "        # create an ongoing list of leaves added to test set\n",
    "        test_set = []\n",
    "\n",
    "        # just pull out the first test_split% (the order of pictures taken was arbitrary, no need to randomize)\n",
    "        image_counter = 0\n",
    "        for ID in map_ID_fileName:\n",
    "            test_set.append(ID)\n",
    "            image_counter += 1\n",
    "            if image_counter >= num_images:\n",
    "                break\n",
    "\n",
    "        # read all images into data\n",
    "        for ID in map_ID_fileName:\n",
    "            \n",
    "            # read with mpimg because opencv messes the colors up for RGB\n",
    "            if color:\n",
    "                img = mpimg.imread(this_folder + \"/\" +  map_ID_fileName[ID]) / 255.\n",
    "            else:\n",
    "                img = rgb2gray(mpimg.imread(this_folder + \"/\" +  map_ID_fileName[ID]) / 255.)\n",
    "            #img = img[80:144, 80:144]\n",
    "            \n",
    "            # save to training or test set\n",
    "            leaf = leaf_map[ID]\n",
    "            dataset = 'train'\n",
    "            if ID in test_set:\n",
    "                dataset = 'test'\n",
    "            if leaf not in data[dataset][class_name]:\n",
    "                data[dataset][class_name][leaf] = {}\n",
    "            data[dataset][class_name][leaf][ID] = img\n",
    "\n",
    "    pickle.dump(data, open(data_path, \"wb\" ) )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate gabor filters\n",
    "def gaborFilters(filter_path):\n",
    "    \n",
    "    # check if file exists already\n",
    "    if path.exists(filter_path):\n",
    "        kernels = pickle.load( open( filter_path, \"rb\" ) )\n",
    "    \n",
    "    # create kernels\n",
    "    else:\n",
    "        kernels = []\n",
    "        for theta in range(4):\n",
    "            theta = theta / 4. * np.pi\n",
    "            for offset in range(4):\n",
    "                offset = offset / 4. * np.pi\n",
    "                for sigma in (1, 3):\n",
    "                    for freq in (0.05, 0.25):\n",
    "                        kernel = np.real(gabor_kernel(freq, offset=offset, theta=theta, sigma_x=sigma, sigma_y=sigma))\n",
    "                        kernels.append(kernel)\n",
    "        pickle.dump(kernels, open(filter_path, \"wb\" ) )\n",
    "    \n",
    "    # plot kernels\n",
    "    i=0\n",
    "    plt.figure(figsize=(len(kernels), len(kernels)))\n",
    "    for kernel in kernels:\n",
    "        plt.subplot(sqrt(len(kernels)) ,sqrt(len(kernels)),i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(kernel, cmap=plt.cm.gray)\n",
    "        #plt.xlabel(str(theta) + '_' + str(offset) + '_' + str(sigma) + '_' + str(frequency))\n",
    "        i+=1\n",
    "    plt.show()\n",
    "    \n",
    "    return kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filters to images\n",
    "from scipy import ndimage as ndi\n",
    "from skimage import data\n",
    "from skimage.util import img_as_float\n",
    "from skimage.filters import gabor_kernel\n",
    "from IPython.display import display, clear_output\n",
    "def applyFilters(data, data_path, filters, color=True):\n",
    "    \n",
    "    # check if file exists already\n",
    "    if path.exists(data_path):\n",
    "        return pickle.load( open( data_path, \"rb\" ) )\n",
    "    \n",
    "    filteredData = {}\n",
    "    for dataset in data:\n",
    "        filteredData[dataset] = {}\n",
    "        for className in data[dataset]:\n",
    "            dis = display('aplying filters to: ' + dataset + ', ' + className, display_id=True)\n",
    "            filteredData[dataset][className] = {}\n",
    "            for idx, leaf in enumerate(data[dataset][className]):\n",
    "                dis.update('aplying filters to: ' + dataset + ', ' + className \n",
    "                           + f' {100. * float(idx) / float(len(data[dataset][className])):.0f}' + \"%\")\n",
    "                filteredData[dataset][className][leaf] = {}\n",
    "                for ID in data[dataset][className][leaf]:\n",
    "                    img = data[dataset][className][leaf][ID]\n",
    "                    temp = [None] * (3 * len(filters))\n",
    "                    for f, ffilter in enumerate(filters):\n",
    "                        filtered = np.empty(1)\n",
    "                        if color:\n",
    "                            for i in range(3):\n",
    "                                filtered = np.append(filtered, ndi.convolve(img[:,:,i], ffilter, mode='wrap'))\n",
    "                        else:\n",
    "                            filtered = ndi.convolve(img, ffilter, mode='wrap')\n",
    "                        temp[3 * f] = filtered.sum()\n",
    "                        temp[3 * f + 1] = filtered.mean()\n",
    "                        temp[3 * f + 2] = filtered.var()\n",
    "                    filteredData[dataset][className][leaf][ID] = temp\n",
    "            dis.update('filters applied to: ' + dataset + ', ' + className)\n",
    "                    \n",
    "    pickle.dump(filteredData, open(data_path, \"wb\" ) )\n",
    "    return filteredData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for machine learning\n",
    "def prepML(data, data_path, code=''):\n",
    "    \n",
    "    # check if file exists already\n",
    "    if path.exists(data_path):\n",
    "        return pickle.load( open( data_path, \"rb\" ) )\n",
    "    \n",
    "    if code == 'wholeImage':\n",
    "        # get dimensions\n",
    "        nTrain = 0\n",
    "        nTest = 0\n",
    "        for dataset in data:\n",
    "            for className in data[dataset]:\n",
    "                for leaf in data[dataset][className]:\n",
    "                    shape = list(next(iter(data[dataset][className][leaf].values())).shape)\n",
    "                    if dataset == 'train':\n",
    "                        nTrain += len(data[dataset][className][leaf])\n",
    "                    else:\n",
    "                        nTest += len(data[dataset][className][leaf])\n",
    "                        \n",
    "                        \n",
    "        # create ml data\n",
    "        mlData = {}\n",
    "        mlData['X_train'] = np.zeros([nTrain] + shape, dtype=float)\n",
    "        mlData['Y_train'] = [None] * nTrain\n",
    "        mlData['L_train'] = [None] * nTrain\n",
    "        mlData['X_test'] = np.zeros([nTest] + shape, dtype=float)\n",
    "        mlData['Y_test'] = [None] * nTest\n",
    "        mlData['L_test'] = [None] * nTest\n",
    "        train_idx = 0\n",
    "        test_idx = 0\n",
    "        for dataset in data:\n",
    "            for className in data[dataset]:\n",
    "                for leaf in data[dataset][className]:\n",
    "                    for ID in data[dataset][className][leaf]:\n",
    "                        if dataset == 'train':\n",
    "                            mlData['X_train'][train_idx, :, :, :] = data[dataset][className][leaf][ID]\n",
    "                            mlData['Y_train'][train_idx] = className\n",
    "                            mlData['L_train'][train_idx] = leaf\n",
    "                            train_idx += 1\n",
    "                        else:\n",
    "                            mlData['X_test'][test_idx, :, :, :] = data[dataset][className][leaf][ID]\n",
    "                            mlData['Y_test'][test_idx] = className\n",
    "                            mlData['L_test'][test_idx] = leaf\n",
    "                            test_idx += 1\n",
    "    elif code == 'VGG':\n",
    "        from keras.applications.vgg16 import preprocess_input\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        # get dimensions\n",
    "        nTrain = 0\n",
    "        nTest = 0\n",
    "        for dataset in data:\n",
    "            for className in data[dataset]:\n",
    "                for leaf in data[dataset][className]:\n",
    "                    shape = list(next(iter(data[dataset][className][leaf].values())).shape)\n",
    "                    if dataset == 'train':\n",
    "                        nTrain += len(data[dataset][className][leaf])\n",
    "                    else:\n",
    "                        nTest += len(data[dataset][className][leaf])\n",
    "                        \n",
    "                        \n",
    "        # create ml data\n",
    "        mlData = {}\n",
    "        mlData['X_train'] = np.zeros([nTrain] + shape, dtype=float)\n",
    "        mlData['Y_train'] = [None] * nTrain\n",
    "        mlData['L_train'] = [None] * nTrain\n",
    "        mlData['X_test'] = np.zeros([nTest] + shape, dtype=float)\n",
    "        mlData['Y_test'] = [None] * nTest\n",
    "        mlData['L_test'] = [None] * nTest\n",
    "        train_idx = 0\n",
    "        test_idx = 0\n",
    "        for dataset in data:\n",
    "            for className in data[dataset]:\n",
    "                for leaf in data[dataset][className]:\n",
    "                    for ID in data[dataset][className][leaf]:\n",
    "                        if dataset == 'train':\n",
    "                            mlData['X_train'][train_idx, :, :, :] = preprocess_input(data[dataset][className][leaf][ID])\n",
    "                            mlData['Y_train'][train_idx] = className\n",
    "                            mlData['L_train'][train_idx] = leaf\n",
    "                            train_idx += 1\n",
    "                        else:\n",
    "                            mlData['X_test'][test_idx, :, :, :] = preprocess_input(data[dataset][className][leaf][ID])\n",
    "                            mlData['Y_test'][test_idx] = className\n",
    "                            mlData['L_test'][test_idx] = leaf\n",
    "                            test_idx += 1\n",
    "        # onehot encode\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(mlData['Y_train'])\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        mlData['Y_train'] = onehot_encoder.fit_transform(integer_encoded)\n",
    "        \n",
    "        integer_encoded = label_encoder.transform(mlData['Y_test'])\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        mlData['Y_test'] = onehot_encoder.transform(integer_encoded)\n",
    "    else:\n",
    "        # get dimensions\n",
    "        nTrain = 0\n",
    "        nTest = 0\n",
    "        nFeatures = 0\n",
    "        for dataset in data:\n",
    "            for className in data[dataset]:\n",
    "                for leaf in data[dataset][className]:\n",
    "                    if nFeatures == 0:\n",
    "                        nFeatures = len(next(iter(data[dataset][className][leaf].values())))\n",
    "                    if dataset == 'train':\n",
    "                        nTrain += len(data[dataset][className][leaf])\n",
    "                    else:\n",
    "                        nTest += len(data[dataset][className][leaf])\n",
    "\n",
    "        # create ml data\n",
    "        mlData = {}\n",
    "        mlData['X_train'] = np.zeros((nTrain, nFeatures), dtype=float)\n",
    "        mlData['Y_train'] = [None] * nTrain\n",
    "        mlData['L_train'] = [None] * nTrain\n",
    "        mlData['ID_train'] = [None] * nTrain\n",
    "        mlData['X_test'] = np.zeros((nTest, nFeatures), dtype=float)\n",
    "        mlData['Y_test'] = [None] * nTest\n",
    "        mlData['L_test'] = [None] * nTest\n",
    "        mlData['ID_test'] = [None] * nTrain\n",
    "        train_idx = 0\n",
    "        test_idx = 0\n",
    "        for dataset in data:\n",
    "            for className in data[dataset]:\n",
    "                for leaf in data[dataset][className]:\n",
    "                    for ID in data[dataset][className][leaf]:\n",
    "                        x = data[dataset][className][leaf][ID]\n",
    "                        if dataset == 'train':\n",
    "                            for idx, x_i in enumerate(x):\n",
    "                                mlData['X_train'][train_idx][idx] = x_i\n",
    "                            mlData['Y_train'][train_idx] = className\n",
    "                            mlData['L_train'][train_idx] = leaf\n",
    "                            mlData['ID_train'][train_idx] = ID\n",
    "                            train_idx += 1\n",
    "                        else:\n",
    "                            for idx, x_i in enumerate(x):\n",
    "                                mlData['X_test'][test_idx][idx] = x_i\n",
    "                            mlData['Y_test'][test_idx] = className\n",
    "                            mlData['L_test'][test_idx] = leaf\n",
    "                            mlData['ID_test'][test_idx] = ID\n",
    "                            test_idx += 1\n",
    "                    \n",
    "    mlData['X_train'] = standardize(mlData['X_train'])\n",
    "    mlData['X_test'] = standardize(mlData['X_test'])\n",
    "    \n",
    "    pickle.dump(mlData, open(data_path, \"wb\" ), protocol=4 )\n",
    "    return mlData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the results returned from crossValidation_class()\n",
    "# @results are results returned from crossValidation_class()\n",
    "# @mlas are list of mlas used in results to output\n",
    "def plotResults_class(results, mlas):\n",
    "    x_labels = []\n",
    "    for mla in mlas:\n",
    "        x_labels.append(mla)\n",
    "        x_labels.append(' ')\n",
    "        x_labels.append(' ')\n",
    "        x_labels.append(' ')\n",
    "    x_nums = [x for x in range(5*len(mlas))]\n",
    "    fig = plt.figure(figsize=(7.5,5))\n",
    "    plt.xticks(x_nums, x_labels, rotation='vertical', fontsize=20)\n",
    "    plt.yticks([x for x in range(70, 101, 5)], fontsize=20)\n",
    "    plt.tick_params(axis='x', length=0)\n",
    "    plt.ylabel('% Classification Accuracy', fontsize=20)\n",
    "    plt.title('5-Fold Cross-Validation', fontsize=20)\n",
    "    plt.grid(axis='y', linewidth=0.4)\n",
    "    plt.ylim([70, 101])\n",
    "    offset = 0\n",
    "    for mla in mlas:\n",
    "        x = [offset for _ in range(len(results[mla]))]\n",
    "        y = 100. * np.array(results[mla])\n",
    "        if int(max(y)) <= 70:\n",
    "            plt.scatter(offset+0, 71, color='red', marker='v')\n",
    "        else:\n",
    "            plt.scatter(x, y, color='green', marker='_', s=1000)\n",
    "        plt.axvline(x=offset, color='grey', linewidth=0.3, alpha=0.3)\n",
    "        offset += 4\n",
    "    fig.text(0, -.20, 'Illustrates distribution of accuracies for each MLA.\\nResults from 100 random runs.\\nA red triangle indicates results below 70% accuracy'\n",
    "             , fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validates by taking entire leaves out each fold\n",
    "from random import randint\n",
    "from sklearn.metrics import accuracy_score\n",
    "def crossValidation(mla, X, Y, L):\n",
    "    leafs = list(set(L))\n",
    "    Y = np.array(Y)\n",
    "    nFolds = 3\n",
    "    nPer = int(len(leafs) / nFolds)\n",
    "    pred_Y = []\n",
    "    test_Y = []\n",
    "    for f in range(nFolds):\n",
    "        fLeafs = []\n",
    "        if f < nFolds - 1:\n",
    "            for r in range(nPer):\n",
    "                rIdx = randint(0, len(leafs)-1)\n",
    "                rLeaf = leafs[rIdx]\n",
    "                fLeafs.append(rLeaf)\n",
    "                leafs.pop(rIdx)\n",
    "        else:\n",
    "            fLeafs = leafs\n",
    "        fRows = []\n",
    "        for fLeaf in fLeafs:\n",
    "            for idx, l in enumerate(L):\n",
    "                if l == fLeaf:\n",
    "                    fRows.append(idx)\n",
    "        \n",
    "        mla.fit(np.delete(X, fRows, 0), np.delete(Y, fRows, 0))\n",
    "        pred_Y = np.concatenate((pred_Y, mla.predict(np.take(X, fRows, 0))))\n",
    "        test_Y = np.concatenate((test_Y, np.take(Y, fRows, 0)))\n",
    "    return accuracy_score(test_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs cross validation of classification MLAs\n",
    "# @name is name of MLA (see if statements below)\n",
    "# @X is matrix of training features\n",
    "# @y are labels\n",
    "# @nIters is the number of times to re-run cross-validation\n",
    "# @nFolds is number of folds in cross-validation\n",
    "# @layers is a list of layer sizes to use for MLP\n",
    "# @disp will display results\n",
    "# returns list with average accuracy for each iteration\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def crossValidation_class(name, X, Y, L, disp=True):\n",
    "    \n",
    "    # create dummy Machine Learning Algorithm (MLA) object\n",
    "    mla = DecisionTreeClassifier()\n",
    "    \n",
    "    # keep track of results\n",
    "    means = [None] * 100\n",
    "    \n",
    "    # get results\n",
    "    if disp:\n",
    "        dis = display('Training ML: ' + name, display_id=True)\n",
    "    for i in range(100):\n",
    "        if disp:\n",
    "            dis.update('Training ML: ' + name + f' {i:.0f}%')\n",
    "         # create MLA with new random seed\n",
    "        if 'DTr' in name:\n",
    "            mla = DecisionTreeClassifier(random_state = i)\n",
    "        elif 'LRe' in name:\n",
    "            mla = LogisticRegression()\n",
    "        elif 'NBa' in name:\n",
    "            mla = GaussianNB()\n",
    "        elif 'RFo' in name:\n",
    "            mla = RandomForestClassifier(random_state = i, n_estimators=100)\n",
    "        elif 'ETr' in name:\n",
    "            mla = ExtraTreesClassifier(random_state = i, n_estimators=100)\n",
    "        elif 'SVM' in name:\n",
    "            mla = SVC(random_state = i, gamma='auto', probability=True)\n",
    "        elif 'KNN' in name:\n",
    "            mla = KNeighborsClassifier(n_neighbors=2)\n",
    "        elif 'MLP' in name:\n",
    "            mla = MLPClassifier(random_state = i, hidden_layer_sizes=[32], max_iter=10000)\n",
    "            \n",
    "        means[i] = crossValidation(mla, X, Y, L)\n",
    "        \n",
    "    if disp:\n",
    "        dis.update('Trained ML: ' + name + ' with ' + str(sum(means) / len(means)) + '% average accuracy')\n",
    "        \n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs cross-validation multiple times for all data and mlas\n",
    "import warnings\n",
    "def mla_class_results(mlas, X, Y, L, results_path, disp=True):\n",
    "    \n",
    "    # check if file exists already\n",
    "    if path.exists(results_path):\n",
    "        results = pickle.load( open( results_path, \"rb\" ) )\n",
    "        \n",
    "    # otherwise rerun results\n",
    "    else:\n",
    "        results = {}\n",
    "\n",
    "        # run new and plot results, if samples dictionary is filled\n",
    "        warnings.filterwarnings('ignore') # ignore warnings that show when models do not converge\n",
    "\n",
    "        for mla in mlas:\n",
    "            results[mla] = crossValidation_class(mla, X, Y, L, disp)\n",
    "\n",
    "        pickle.dump(results, open(results_path, \"wb\" ) )\n",
    "       \n",
    "    #if disp: \n",
    "        #plotResults_class(results, mlas) \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from resizeimage import resizeimage\n",
    "def resizeImgs(data, width, data_path, color=False): \n",
    "    \n",
    "    # check if file exists already\n",
    "    if path.exists(data_path):\n",
    "        return pickle.load( open( data_path, \"rb\" ) )\n",
    "    \n",
    "    resized = {}\n",
    "    for dataset in data:\n",
    "        resized[dataset] = {}\n",
    "        for className in data[dataset]:\n",
    "            dis = display('resizing: ' + dataset + ', ' + className, display_id=True)\n",
    "            resized[dataset][className] = {}\n",
    "            for idx, leaf in enumerate(data[dataset][className]):\n",
    "                dis.update('resizing: ' + dataset + ', ' + className \n",
    "                           + f' {100. * float(idx) / float(len(data[dataset][className])):.0f}' + \"%\")\n",
    "                resized[dataset][className][leaf] = {}\n",
    "                for ID in data[dataset][className][leaf]:\n",
    "                    if color:\n",
    "                        resized[dataset][className][leaf][ID] = np.zeros((width, width, 3), dtype=float)\n",
    "                        for i in range(3):\n",
    "                            img = Image.fromarray(data[dataset][className][leaf][ID][:,:,i])\n",
    "                            resized[dataset][className][leaf][ID][:,:,i] = np.array(resizeimage.resize_cover(img, [width, width], validate=False))\n",
    "                    else:\n",
    "                        img = Image.fromarray(data[dataset][className][leaf][ID])\n",
    "                        resized[dataset][className][leaf][ID] = np.array(resizeimage.resize_cover(img, [width, width], validate=False))\n",
    "            dis.update('resized: ' + dataset + ', ' + className)\n",
    "    \n",
    "    pickle.dump(resized, open(data_path, \"wb\" ) )\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGT(ID, data):\n",
    "    for dataset in data:\n",
    "        for className in data[dataset]:\n",
    "            for leaf in data[dataset][className]:\n",
    "                for ID2 in data[dataset][className][leaf]:\n",
    "                    if ID2 == ID:\n",
    "                        return className\n",
    "    return 'null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def hardVoter(ensemble_votes, data):\n",
    "    agg_votes = {}\n",
    "    agg_GT = {}\n",
    "    for key in ensemble_votes:\n",
    "        local_votes = []\n",
    "        local_GT = []\n",
    "        for ID in ensemble_votes[key]:\n",
    "            gt = getGT(ID, data)\n",
    "            if gt == 'null':\n",
    "                print('NULL')\n",
    "            pred = ensemble_votes[key][ID]\n",
    "            local_votes.append(pred)\n",
    "            local_GT.append(gt)\n",
    "            if ID not in agg_votes:\n",
    "                agg_votes[ID] = []\n",
    "                agg_GT[ID] = gt\n",
    "            agg_votes[ID].append(pred)\n",
    "        print(key, accuracy_score(local_votes, local_GT))\n",
    "    preds = []\n",
    "    gts = []\n",
    "    missed =[]\n",
    "    for agg in agg_votes:\n",
    "        if len(agg_votes[agg]) != 4:\n",
    "            continue\n",
    "        most_common,num_most_common = Counter(agg_votes[agg]).most_common(1)[0]\n",
    "        print(agg, agg_GT[agg], most_common, agg_votes[agg])\n",
    "        preds.append(most_common)\n",
    "        gts.append(agg_GT[agg])\n",
    "        if most_common != agg_GT[agg]:\n",
    "            missed.append(agg)\n",
    "    return preds, gts, missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def viewIDImages(data, missed, color=True):\n",
    "    # data[className][leafNumber][ID]\n",
    "    nImages = len(missed)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(nImages):\n",
    "        plt.subplot(ceil(sqrt(nImages)), ceil(sqrt(nImages)),i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        ID = missed[i]\n",
    "        for dataset in data:\n",
    "            for className in data[dataset]:\n",
    "                for leaf in data[dataset][className]:\n",
    "                    for ID2 in data[dataset][className][leaf]:\n",
    "                        if ID2 == ID:\n",
    "                            if color:\n",
    "                                plt.imshow(data[dataset][className][leaf][ID])\n",
    "                            else:\n",
    "                                plt.imshow(data[dataset][className][leaf][ID], cmap=plt.cm.gray)\n",
    "                            plt.xlabel(className + '_' + str(ID))\n",
    "                            break\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
